{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9a4bac",
   "metadata": {},
   "source": [
    "\n",
    "# Data Analysis for Pyannote Diarization Protocol\n",
    "\n",
    "This notebook helps you **verify and analyze** your dataset used with `pyannote.audio`:\n",
    "1. **Verify** all audio files in each split (`train/dev/test`) can be loaded.\n",
    "2. Show **split sizes** (file counts).\n",
    "3. Compute **durations** (per split + total) and print in `H M S`.\n",
    "4. Compute **speakers-per-chunk** statistics (and the **max** across all chunks).\n",
    "5. Check **sample rate (frequency)** distribution and **silence ratio** per file (with a histogram).\n",
    "6. Visualize **spectrograms** for 15 random training files.\n",
    "7. Apply **MUSAN augmentation** (music, noise, babble) on those 15 files with the **same SNR ranges** as training code, plot spectrograms, and **save** augmented WAVs to `./augmented_samples`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports & settings ---\n",
    "import os, math, random, warnings, json, glob\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyannote.database import registry\n",
    "from pyannote.core import Segment\n",
    "\n",
    "# Reduce noisy deprecation warnings seen in recent torchaudio/pandas.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\".*torchaudio\\._backend.*deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=r\".*delim_whitespace.*\")\n",
    "\n",
    "# Make plots a bit larger by default\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__, \"| torchaudio:\", torchaudio.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Environment variable PYANNOTE_DATABASE_CONFIG must point to your data/database.yml\n",
    "db_cfg = os.environ.get(\"PYANNOTE_DATABASE_CONFIG\")\n",
    "assert db_cfg and Path(db_cfg).exists(), \"Set PYANNOTE_DATABASE_CONFIG to your data/database.yml\"\n",
    "\n",
    "# Your protocol name:\n",
    "PROTOCOL = \"MyDatabase.SpeakerDiarization.MyProtocol\"\n",
    "\n",
    "# Chunk duration (seconds) used to count speakers-per-chunk (match your training, e.g., 2.0)\n",
    "CHUNK_DURATION = 2.0\n",
    "\n",
    "# For MUSAN augmentation (same defaults as your training script)\n",
    "MUSAN_ROOT = Path(os.environ.get(\"MUSAN_ROOT\", \"/musan\"))\n",
    "SNR_NOISE = (5.0, 20.0)\n",
    "SNR_MUSIC = (5.0, 20.0)\n",
    "SNR_BABBLE = (10.0, 20.0)\n",
    "\n",
    "# Probabilities (weights) for selecting a background type when augmenting\n",
    "P_NOISE = 0.4\n",
    "P_MUSIC = 0.4\n",
    "P_BABBLE = 0.4\n",
    "\n",
    "# Safety limits for heavy computations\n",
    "MAX_FILES_FOR_CHUNK_ANALYSIS = None   # set to an int (e.g., 200) to speed up\n",
    "MAX_FILES_FOR_SPECTRUM = 100          # PSD averaging cap for speed\n",
    "N_SPECTROGRAM_SAMPLES = 15            # how many training files to visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper utilities ---\n",
    "def hms(seconds: float) -> str:\n",
    "    seconds = int(round(seconds))\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return f\"{h}h {m}m {s}s\"\n",
    "\n",
    "def audio_path_pattern_from_cfg(cfg_path: str) -> str:\n",
    "    '''Assumes audio lives next to database.yml under ./audio/{uri}.wav\n",
    "    Adjust if your layout differs.\n",
    "    '''\n",
    "    base = Path(cfg_path).parent\n",
    "    return str(base / \"audio\" / \"{uri}.wav\")\n",
    "\n",
    "class UriToAudioPath:\n",
    "    def __init__(self, pattern: str):\n",
    "        self.pattern = pattern\n",
    "    def __call__(self, file):\n",
    "        return self.pattern.format(uri=file[\"uri\"])\n",
    "\n",
    "def safe_audio_info(path: str):\n",
    "    '''Return (num_frames, sample_rate, num_channels) without fully loading audio.'''\n",
    "    try:\n",
    "        info = torchaudio.info(path)\n",
    "        return info.num_frames, info.sample_rate, info.num_channels\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_wave(path: str, target_sr: Optional[int] = None):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    wav = wav.mean(dim=0, keepdim=True) if wav.shape[0] > 1 else wav[:1, :]\n",
    "    if target_sr and sr != target_sr:\n",
    "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "        sr = target_sr\n",
    "    return wav, sr\n",
    "\n",
    "def compute_silence_ratio(samples: torch.Tensor, sr: int, frame_ms: int = 20, hop_ms: int = 10, threshold_db: float = -40.0) -> float:\n",
    "    '''Fraction of frames whose RMS (dBFS) < threshold_db. samples: (1, T).'''\n",
    "    x = samples.squeeze(0).cpu().numpy()\n",
    "    frame = int(sr * frame_ms / 1000.0)\n",
    "    hop = int(sr * hop_ms / 1000.0)\n",
    "    if frame <= 0: frame = 1\n",
    "    if hop <= 0: hop = 1\n",
    "\n",
    "    silent = 0\n",
    "    total = 0\n",
    "    # Ensure at least one step even for short files\n",
    "    end_idx = max(0, len(x) - frame + 1)\n",
    "    if end_idx == 0 and len(x) >= 1:\n",
    "        end_idx = 1\n",
    "    for start in range(0, end_idx, hop):\n",
    "        seg = x[start:start+frame]\n",
    "        if len(seg) == 0:\n",
    "            continue\n",
    "        rms = np.sqrt(np.mean(seg**2) + 1e-12)\n",
    "        db = 20.0 * np.log10(rms + 1e-12)\n",
    "        if db < threshold_db:\n",
    "            silent += 1\n",
    "        total += 1\n",
    "    return (silent / total) if total else 0.0\n",
    "\n",
    "def plot_spectrogram(samples: torch.Tensor, sr: int, title: str = \"Spectrogram\"):\n",
    "    # Compute magnitude spectrogram via STFT\n",
    "    n_fft = 1024\n",
    "    hop_length = 256\n",
    "    spec = torch.stft(samples.squeeze(0), n_fft=n_fft, hop_length=hop_length, return_complex=True)\n",
    "    mag = spec.abs().numpy()\n",
    "    mag_db = 20 * np.log10(mag + 1e-10)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.imshow(mag_db, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"dB\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(\"Frequency bins\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def pick_random_segment(wav: torch.Tensor, target_len: int) -> torch.Tensor:\n",
    "    '''wav: (1, T) on CPU; returns (1, target_len), looping if too short.'''\n",
    "    T = wav.shape[-1]\n",
    "    if T >= target_len:\n",
    "        start = random.randint(0, T - target_len)\n",
    "        return wav[:, start:start+target_len]\n",
    "    reps = max(1, int(math.ceil(target_len / max(1, T))))\n",
    "    out = wav.repeat(1, reps)[:, :target_len]\n",
    "    return out\n",
    "\n",
    "def mix_at_snr(clean: torch.Tensor, noise: torch.Tensor, snr_db: float) -> torch.Tensor:\n",
    "    '''Both shape (1, T), returns mixed at target SNR (approx) and clipped to [-1,1].'''\n",
    "    Px = max(np.sqrt(float((clean**2).mean())), 1e-12)\n",
    "    Pn = max(np.sqrt(float((noise**2).mean())), 1e-12)\n",
    "    snr_lin = 10.0 ** (snr_db / 10.0)\n",
    "    a = Px / (Pn * math.sqrt(snr_lin))\n",
    "    out = clean + a * noise\n",
    "    return torch.clamp(out, -1.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ac03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load protocol with preprocessors ---\n",
    "audio_pattern = audio_path_pattern_from_cfg(db_cfg)\n",
    "pre = {\"audio\": UriToAudioPath(audio_pattern)}\n",
    "registry.load_database(db_cfg)\n",
    "proto = registry.get_protocol(PROTOCOL, preprocessors=pre)\n",
    "\n",
    "def collect_split(split_iter):\n",
    "    items = []\n",
    "    for f in split_iter:\n",
    "        items.append(f)\n",
    "    return items\n",
    "\n",
    "train_items = collect_split(proto.train())\n",
    "dev_items   = collect_split(proto.development())\n",
    "test_items  = collect_split(proto.test())\n",
    "\n",
    "len(train_items), len(dev_items), len(test_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb34a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1) Check all data loads successfully & 2/3) Splits and durations ---\n",
    "def verify_and_sum(items, name: str):\n",
    "    ok = 0\n",
    "    missing = 0\n",
    "    total_sec = 0.0\n",
    "    sr_hist = {}\n",
    "    bad_files = []\n",
    "    for it in items:\n",
    "        path = it[\"audio\"]\n",
    "        info = safe_audio_info(path)\n",
    "        if info is None:\n",
    "            missing += 1\n",
    "            bad_files.append(path)\n",
    "            continue\n",
    "        num_frames, sr, ch = info\n",
    "        dur = float(num_frames) / float(sr) if sr > 0 else 0.0\n",
    "        total_sec += dur\n",
    "        sr_hist[sr] = sr_hist.get(sr, 0) + 1\n",
    "        ok += 1\n",
    "    print(f\"{name}: {ok}/{len(items)} files OK | audio={hms(total_sec)} | missing={missing}\")\n",
    "    if bad_files:\n",
    "        print(\"  Missing/unreadable example:\", bad_files[:5])\n",
    "    return total_sec, sr_hist\n",
    "\n",
    "train_sec, train_sr_hist = verify_and_sum(train_items, \"train\")\n",
    "dev_sec,   dev_sr_hist   = verify_and_sum(dev_items,   \"dev\")\n",
    "test_sec,  test_sr_hist  = verify_and_sum(test_items,  \"test\")\n",
    "\n",
    "print(\"\\nSplit sizes -> train/dev/test:\", len(train_items), len(dev_items), len(test_items))\n",
    "print(\"Total duration:\", hms(train_sec + dev_sec + test_sec))\n",
    "\n",
    "# Merge SR histograms for a global view\n",
    "from collections import Counter\n",
    "sr_counter = Counter()\n",
    "sr_counter.update(train_sr_hist)\n",
    "sr_counter.update(dev_sr_hist)\n",
    "sr_counter.update(test_sr_hist)\n",
    "\n",
    "print(\"\\nSample-rate distribution (all splits):\")\n",
    "for sr, cnt in sorted(sr_counter.items()):\n",
    "    print(f\"  {sr} Hz: {cnt} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d417256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4) Speakers per CHUNK (e.g., 2 seconds) on TRAIN ---\n",
    "def speakers_per_chunk_for_file(item, chunk_duration: float):\n",
    "    annotation = item[\"annotation\"]\n",
    "    info = safe_audio_info(item[\"audio\"])\n",
    "    if info is None:\n",
    "        return []\n",
    "    num_frames, sr, _ = info\n",
    "    file_dur = float(num_frames) / float(sr) if sr > 0 else 0.0\n",
    "\n",
    "    counts = []\n",
    "    start = 0.0\n",
    "    while start < file_dur - 1e-6:\n",
    "        end = min(file_dur, start + chunk_duration)\n",
    "        seg = Segment(start, end)\n",
    "        cropped = annotation.crop(seg, mode=\"intersection\", return_timeline=False)\n",
    "        labels = set()\n",
    "        for (s, t, label) in cropped.itertracks(yield_label=True):\n",
    "            labels.add(label)\n",
    "        counts.append(len(labels))\n",
    "        start += chunk_duration\n",
    "    return counts\n",
    "\n",
    "subset = train_items if (MAX_FILES_FOR_CHUNK_ANALYSIS is None) else train_items[:MAX_FILES_FOR_CHUNK_ANALYSIS]\n",
    "all_counts = []\n",
    "for it in subset:\n",
    "    all_counts.extend(speakers_per_chunk_for_file(it, CHUNK_DURATION))\n",
    "\n",
    "if all_counts:\n",
    "    print(f\"Max speakers in any {CHUNK_DURATION:.1f}s chunk (train):\", max(all_counts))\n",
    "    # Quick histogram printout\n",
    "    hist = {}\n",
    "    for c in all_counts:\n",
    "        hist[c] = hist.get(c, 0) + 1\n",
    "    print(\"Speakers-per-chunk histogram (count of chunks):\", hist)\n",
    "else:\n",
    "    print(\"No counts computed (maybe items unreadable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5) Plot sample-rate distribution & silence ratio on TRAIN ---\n",
    "# Plot SR histogram:\n",
    "all_sr = []\n",
    "for sr, cnt in train_sr_hist.items():\n",
    "    all_sr.extend([sr]*cnt)\n",
    "\n",
    "if all_sr:\n",
    "    plt.figure()\n",
    "    plt.hist(all_sr, bins=len(set(all_sr)))\n",
    "    plt.title(\"Training sample-rate distribution (Hz)\")\n",
    "    plt.xlabel(\"Sample rate (Hz)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sample-rate data to plot.\")\n",
    "\n",
    "# Silence ratio per file (train)\n",
    "silence_rows = []\n",
    "for it in train_items:\n",
    "    info = safe_audio_info(it[\"audio\"])\n",
    "    if info is None:\n",
    "        continue\n",
    "    wav, sr = load_wave(it[\"audio\"], target_sr=None)\n",
    "    ratio = compute_silence_ratio(wav, sr, frame_ms=20, hop_ms=10, threshold_db=-40.0)\n",
    "    silence_rows.append((it[\"uri\"], ratio))\n",
    "\n",
    "# Report\n",
    "if silence_rows:\n",
    "    avg_ratio = float(np.mean([r for _, r in silence_rows]))\n",
    "    print(f\"Average silence ratio (per file) in TRAIN: {avg_ratio*100:.2f}%\")\n",
    "    # Plot histogram of ratios\n",
    "    plt.figure()\n",
    "    plt.hist([r for _, r in silence_rows], bins=30)\n",
    "    plt.title(\"Silence ratio per training file\")\n",
    "    plt.xlabel(\"Fraction of silent frames\")\n",
    "    plt.ylabel(\"Files\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Top 10 most-silent files\n",
    "    top10 = sorted(silence_rows, key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Top 10 most-silent training files:\")\n",
    "    for uri, r in top10:\n",
    "        print(f\"  {uri}: {r*100:.2f}% silent\")\n",
    "else:\n",
    "    print(\"No silence data computed (no train files loaded).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a39e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6) Average frequency content (Welch PSD) on a subset of TRAIN ---\n",
    "try:\n",
    "    from scipy.signal import welch\n",
    "    subset_for_psd = train_items[:min(len(train_items), MAX_FILES_FOR_SPECTRUM)]\n",
    "    psd_sum = None\n",
    "    freqs_ref = None\n",
    "    count = 0\n",
    "\n",
    "    for it in subset_for_psd:\n",
    "        info = safe_audio_info(it[\"audio\"])\n",
    "        if info is None:\n",
    "            continue\n",
    "        wav, sr = load_wave(it[\"audio\"], target_sr=None)\n",
    "        x = wav.squeeze(0).numpy()\n",
    "        if len(x) < 2048:\n",
    "            continue\n",
    "        f, Pxx = welch(x, fs=sr, nperseg=2048)\n",
    "        if psd_sum is None:\n",
    "            psd_sum = Pxx\n",
    "            freqs_ref = f\n",
    "        else:\n",
    "            m = min(len(psd_sum), len(Pxx))\n",
    "            psd_sum[:m] += Pxx[:m]\n",
    "        count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        psd_avg = psd_sum / count\n",
    "        plt.figure()\n",
    "        plt.semilogy(freqs_ref[:len(psd_avg)], psd_avg)\n",
    "        plt.title(\"Average Welch PSD (training subset)\")\n",
    "        plt.xlabel(\"Frequency (Hz)\")\n",
    "        plt.ylabel(\"Power spectral density\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No PSD plotted (no valid training audio).\")\n",
    "except Exception as e:\n",
    "    print(\"Skipping PSD plot (scipy not available or other error):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 7) Spectrograms for random 15 training files ---\n",
    "N = min(N_SPECTROGRAM_SAMPLES, len(train_items))\n",
    "if N == 0:\n",
    "    print(\"No training items to visualize.\")\n",
    "else:\n",
    "    chosen = random.sample(train_items, k=N)\n",
    "    for it in chosen:\n",
    "        path = it[\"audio\"]\n",
    "        info = safe_audio_info(path)\n",
    "        if info is None:\n",
    "            print(\"Unreadable:\", path)\n",
    "            continue\n",
    "        wav, sr = load_wave(path, target_sr=None)\n",
    "        title = f\"{it['uri']} (sr={sr})\"\n",
    "        plot_spectrogram(wav, sr, title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 8) MUSAN augmentation (music, noise, babble) on the same files ---\n",
    "def list_audio_files(root: Path):\n",
    "    if not root.exists():\n",
    "        return []\n",
    "    exts = (\".wav\", \".flac\", \".mp3\", \".ogg\")\n",
    "    return [Path(p) for p in glob.glob(str(root / \"**\" / \"*\"), recursive=True) if Path(p).suffix.lower() in exts]\n",
    "\n",
    "def load_random_bg(files: List[Path], target_len: int, sr: int):\n",
    "    if not files:\n",
    "        return None, None\n",
    "    path = random.choice(files)\n",
    "    wav, file_sr = torchaudio.load(str(path))\n",
    "    wav = wav.mean(dim=0, keepdim=True) if wav.shape[0] > 1 else wav[:1, :]\n",
    "    if file_sr != sr:\n",
    "        wav = torchaudio.functional.resample(wav, file_sr, sr)\n",
    "        file_sr = sr\n",
    "    noise = pick_random_segment(wav, target_len)\n",
    "    return noise, path\n",
    "\n",
    "musan_noise  = list_audio_files(MUSAN_ROOT / \"noise\")\n",
    "musan_music  = list_audio_files(MUSAN_ROOT / \"music\")\n",
    "musan_speech = list_audio_files(MUSAN_ROOT / \"speech\")\n",
    "\n",
    "print(f\"MUSAN pools -> noise={len(musan_noise)}, music={len(musan_music)}, babble(speech)={len(musan_speech)}\")\n",
    "\n",
    "out_dir = Path(\"./augmented_samples\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'chosen' not in globals() or not chosen:\n",
    "    print(\"No previously chosen files from training set; skipping augmentation cell.\")\n",
    "else:\n",
    "    for it in chosen:\n",
    "        path = it[\"audio\"]\n",
    "        info = safe_audio_info(path)\n",
    "        if info is None:\n",
    "            print(\"Unreadable:\", path)\n",
    "            continue\n",
    "        wav, sr = load_wave(path, target_sr=None)\n",
    "        T = wav.shape[-1]\n",
    "\n",
    "        jobs = [\n",
    "            (\"music\",  musan_music,  SNR_MUSIC),\n",
    "            (\"noise\",  musan_noise,  SNR_NOISE),\n",
    "            (\"babble\", musan_speech, SNR_BABBLE),\n",
    "        ]\n",
    "\n",
    "        for kind, pool, (lo, hi) in jobs:\n",
    "            if not pool:\n",
    "                print(f\"Skipping {kind}: no files found under MUSAN.\")\n",
    "                continue\n",
    "            bg, bg_path = load_random_bg(pool, T, sr)\n",
    "            if bg is None:\n",
    "                print(f\"Skipping {kind}: failed to load bg.\")\n",
    "                continue\n",
    "            snr = random.uniform(lo, hi)\n",
    "            mixed = mix_at_snr(wav, bg, snr_db=snr)\n",
    "\n",
    "            # Save\n",
    "            out_path = out_dir / f\"{it['uri']}_{kind}.wav\"\n",
    "            torchaudio.save(str(out_path), mixed, sr)\n",
    "            print(f\"Saved: {out_path} (SNR≈{snr:.1f} dB)\")\n",
    "\n",
    "            # Plot\n",
    "            plot_spectrogram(mixed, sr, title=f\"{it['uri']} + {kind} (SNR≈{snr:.1f} dB)\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
